# CONSTRUÇÃO DE UM PIPELINE DE DADOS INTEGRADO NA NUVEM AZURE USANDO O BANCO DE DADOS
A era da informação trouxe consigo um desafio significativo para as organizações, que agora enfrentam a tarefa complexa de extrair informações valiosas de volumes massivos de dados, ao mesmo tempo em que buscam garantir a eficiência na gestão desses dados. A crescente quantidade de dados gerada no mundo contemporâneo destaca a necessidade crítica de transformar dados em insights úteis para embasar a tomada de decisões nos negócios.
Nesse contexto em constante evolução, as organizações estão buscando abordagens e tecnologias que lhes permitam lidar com o crescimento exponencial de dados e, ao mesmo tempo, extrair valor estratégico a partir deles. Este estudo é resultado das experiências vividas pelos autores desta pesquisa, profissionais atuantes no campo de engenharia de dados nas empresas X e Y. 
O objetivo deste artigo é demonstrar uma rotina de integração de dados na nuvem, fazendo uso de pipelines de dados. O cenário em que esta rotina será contextualizada se encontra em um ambiente de estudo hospedado na plataforma de cloud Microsoft Azure. O estudo destacará as diversas camadas de processamento de dados dentro do pipeline. 
A primeira etapa do pipeline é responsável pela ingestão dos dados brutos, preservando a sua forma original a partir da fonte, e os armazena na primeira camada do Data Lake. Após essa etapa, inicia-se a integração dos dados na segunda camada, na qual as informações são armazenadas com a última atualização gerada na primeira camada. Nesta etapa, os dados são mantidos sem transformações. A terceira camada é a próxima etapa do pipeline, onde ocorrem as transformações dos dados. Aqui, são realizados "inner Join" nas bases de dados do banco Adventure Works, e os dados são enriquecidos com regras específicas. O resultado desta etapa é a preparação dos dados da terceira camada, tornando-os prontos para o consumo em aplicações de Machine Learning.
Para a criação deste pipeline, a arquitetura selecionada inclui as seguintes etapas e tecnologias: banco de dados Microsoft SQL Serve que será a fonte de origem dos dados, usando a base de dados Adventure Works. O Azure Data Factory que fará a parte de ingestão dos dados. Será usado o sistema de arquivo distribuído da Azure o Data Lake Storage Gen2. E o processamento dos dados será realizado no Azure Databricks (utilizando SPARK) e a organização das camadas no Delta Lake.
Este modelo descreve um tipo de arquitetura e o fluxo de dados, também demonstra a eficácia prática de tal abordagem na integração e processamento de informações na nuvem. Através da análise do pipeline proposto, este estudo busca oferecer insights para aqueles que buscam soluções para os desafios crescentes da gestão de dados.
